# Sign-Language-using-Hand-Gesture-Recognition
In this project, we have developed an environment that recognizes static gestures of English alphabet according to the conventions of the American Sign Language (except J and Z). The sign language considered here is a subset of gestures made with hand. This system is based on appearance based vision recognition system. The main objective of this project is to develop a low cost manner to recognize the hand gesture as the popular methods include the usage of wired gloves and depth aware cameras which are inconvenient for the users. The work flow of this application is: capture the gesture via webcam (real-time video sequence), preprocess it according to the classification method used, extract relevant features (Hu moments and Speeded-Up Robust Features, SURF) and feed it to the classifier and generate the output showcasing the letter corresponding to the gesture. This represents the conversion of gesture to text. We also convert text to gesture for sentences typed in. For deep learning, preprocesing of the image and feature extraction is automatically done by the system. We also bring out a comparative analysis of different classification methods - Logistic Regression, Support Vector Machine, Naive Bayesian, Multi-layer Perceptron and Deep Learning under the feature extraction methods Hu moments and SURF.
